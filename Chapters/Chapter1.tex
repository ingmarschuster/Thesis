\chapter{Introduction}

\section{Distributional Semantics}
\section{Approaches to phrase modelling in distributional semantics}
\section{An approach based on probability theory}
\begin{itemize}
	\item Advantages of using probability theory (simplicity of model, generative perspective, no ad-hocery)
	\item Bayesian Inference as straight forward way to fit generative models
\end{itemize}
\section{Bayesian Nonparametrics}
\begin{itemize}
	\item Advantage: model complexity is adjusted to the data
\end{itemize}

\chapter{A Linear approach: Probabilistic Matrix Factorization}
matrix factorization paper

\chapter{MAYBE: Extending the linear approach to a model of Classical conditioning}
Linear approach can be interpreted as a model of classical conditioning akin to the Rescorla-Wagner model. It can be extended to allow more than one combination of cue and outcome in a similar fashion as the Rescorla-Wagner model.

\chapter{Bayesian Nonparametric continuous priors}
In the matrix factorization paper it becomes evident that a dimensionality-mixture for continuous latent variables would be desirable. Also, infinite-dimensional nonparametric priors have shortcomings (cf. Jeffs papers/thesis). Thus we develop our own nonparametric continuous prior.
\section{Shortcomings of infinite-dimensional priors}
\section{Existing techniques to sample from dimensionality-mixture models}
Reversible Jump, evidence-based
\section{A prior for nonparametric continuous latent variables}

\chapter{A nonlinear approach to Matrix factorization using Gaussian Processes}
\begin{itemize}
\item Advantages 
	\begin{itemize}
		\item comparison of phrases which differing number of words possible as opposed to linear model - thanks to covariance function and matrix norm
		\item nonlinear model means lower dimensionality for latent variables
	\end{itemize}
\item Problem: complex implementation
\end{itemize}
\section{Implementation for modelling directed, weighted graphs}
Every edge contains two words (nodes) and is weighted with their cooccurence count (edge weight).
\section{Implementation for modelling directed, weighted hypergraphs}
An edge contains an arbitrary number of words (nodes) and is weighted with their cooccurence count (edge weight).

\chapter{Conclusion and outlook}